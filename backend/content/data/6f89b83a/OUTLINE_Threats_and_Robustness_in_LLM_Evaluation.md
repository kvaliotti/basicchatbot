1. Introduction to the study's focus on vulnerabilities in LLM-based evaluation systems.
2. Overview of the models evaluated: specialized reward models (including proposed Master-RM) and general-purpose LLMs like GPT-4o, LLaMA-3, Qwen2.5.
3. Description of the benchmark: 2,500 mixed reasoning examples from datasets such as Multi-subject RLVR, NaturalReasoning, and MATH.
4. Adversarial attack techniques used: simple prompts like 'thought process,' 'solution,' and multilingual tokens like 'è§£.'
5. Evaluation metrics: measuring false positive rates (FPR).
6. Key findings: high susceptibility of models like GPT-4o, with FPRs up to 66.8%.
7. Performance of Master-RM: near-zero FPR (~0%) across datasets, demonstrating robustness.
8. Implications: current evaluation models are vulnerable, posing systematic risks.
9. Limitations of existing defenses: superficial methods like chain-of-thought and voting are ineffective against hacking.
10. Practical significance: the necessity for robust evaluators, highlighting the potential of Master-RM for reliable AI judgment.
11. Conclusion: emphasizing the urgent need to improve robustness of LLM evaluation systems to prevent manipulation.
