1. Introduction to LLM-based evaluators and their growing role in AI assessments.
2. Key vulnerabilities: ease of manipulation via prompt engineering, the 'master key' attack.
3. Methodology: crafting adversarial prompts by truncation and generic phrases.
4. Experimental results: high false positive rates across models and benchmarks, susceptibility to prompt attacks.
5. Introduction of Master-RM: a robust evaluation framework resistant to manipulation.
6. Performance analysis of Master-RM: high agreement with GPT-4o and resilience under attack.
7. Implications for AI safety and trustworthiness: the need for robust evaluation systems.
8. Concluding remarks: advancing toward secure and reliable AI evaluation tools.
