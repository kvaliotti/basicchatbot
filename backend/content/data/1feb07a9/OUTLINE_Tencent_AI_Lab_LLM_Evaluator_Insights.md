1. Introduction: Highlighting the importance of reliable LLM evaluators in AI applications.
2. Overview of Tencent AI Lab's recent findings on LLM evaluator performance.
3. 1. High Agreement with Human Judgments: Detailing 80%+ agreement levels across reasoning, coding, and decision-making benchmarks.
4. 2. Vulnerabilities to Prompt Manipulation: Explaining how superficial prompt tweaks lead to false positives and expose robustness weaknesses.
5. 3. Adversarial Training for Robustness: Describing synthetic adversarial response techniques, the Master-RM model, and resulting resilience improvements.
6. 4. Methodology Summary: Models tested (GPT-4, Claude 4.0, LLaMA series), tasks evaluated, and use of fixed inference parameters.
7. 5. Implications: Importance of adversarial training for AI safety, education, and hiring; enhancing trust and fairness in automated evaluation.
8. Conclusion: Future directions and the necessity of continued research to fortify LLM evaluators against adversarial risks.
